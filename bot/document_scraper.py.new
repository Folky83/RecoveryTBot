"""
Document Scraper for Mintos Companies
Handles scraping and monitoring of company document pages for new documents.
"""
import os
import json
import time
import logging
from typing import Dict, List, Optional, Any, Union

from .logger import setup_logger
from .url_fetcher import URLFetcher
from .document_parser import DocumentParser

logger = setup_logger(__name__)

class DocumentScraper:
    """Scrapes and monitors company document pages for new documents"""
    
    CACHE_DIR = "data"
    DOCUMENTS_CACHE_FILE = os.path.join(CACHE_DIR, "documents_cache.json")
    COMPANY_URLS_CACHE_FILE = os.path.join(CACHE_DIR, "company_urls_cache.json")
    FALLBACK_MAPPING_FILE = os.path.join(CACHE_DIR, "company_fallback_mapping.json")
    
    def __init__(self):
        """Initialize document scraper with required components"""
        # Create cache directory if it doesn't exist
        os.makedirs(self.CACHE_DIR, exist_ok=True)
        
        # Initialize components
        self.url_fetcher = URLFetcher()
        self.document_parser = DocumentParser()
        
        # Load previous document data
        self.documents_data = self._load_documents_data()
        
    def _load_documents_data(self) -> Dict[str, Any]:
        """Load documents data from file"""
        try:
            if os.path.exists(self.DOCUMENTS_CACHE_FILE):
                with open(self.DOCUMENTS_CACHE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"Loaded documents data: {len(data)} companies")
                return data
            else:
                logger.info("No document cache file found, creating a new one")
                return {}
        except Exception as e:
            logger.error(f"Error loading documents data: {e}", exc_info=True)
            return {}
            
    def _save_documents_data(self) -> None:
        """Save documents data to file"""
        try:
            with open(self.DOCUMENTS_CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.documents_data, f, indent=2)
            logger.info(f"Saved documents data: {len(self.documents_data)} companies")
        except Exception as e:
            logger.error(f"Error saving documents data: {e}", exc_info=True)
    
    def fetch_company_urls(self) -> Dict[str, Dict[str, str]]:
        """Fetch company URLs from the Mintos lending companies details page
        
        Returns:
            Dictionary mapping company IDs to {name, url} dictionaries
        """
        return self.url_fetcher.fetch_company_urls()
    
    def _load_company_fallback_mapping(self) -> Dict[str, Dict[str, str]]:
        """Load company fallback mapping from JSON file
        
        This mapping provides alternative company IDs for companies that have
        been renamed or have special URL patterns.
        
        Returns:
            Dictionary mapping original company IDs to alternative IDs
        """
        return self.url_fetcher.load_company_fallback_mapping()
    
    def _generate_url_variations(self, company_id: str, company_name: str) -> List[str]:
        """Generate multiple URL variations for a company
        
        Args:
            company_id: The ID of the company
            company_name: The name of the company
            
        Returns:
            List of possible URLs to try
        """
        return self.url_fetcher.generate_url_variations(company_id, company_name)
    
    def get_company_documents(self, company_id: str, company_name: str, company_url: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get documents for a specific company
        
        Args:
            company_id: The ID of the company
            company_name: The name of the company
            company_url: Optional direct URL to the company page
            
        Returns:
            List of documents for the company
        """
        logger.info(f"Fetching documents for {company_name} (ID: {company_id})")
        
        # URLs to try
        tried_urls = set()
        
        # First try the direct URL if provided
        if company_url:
            tried_urls.add(company_url)
            logger.info(f"Trying direct URL: {company_url}")
            
            # Try with JavaScript rendering first
            html_content = self.url_fetcher.make_request(company_url, use_js_rendering=True)
            if html_content:
                logger.info(f"Successfully fetched content from {company_url} with JS rendering")
                documents = self.document_parser.parse_documents(html_content, company_name)
                if documents:
                    logger.info(f"Found {len(documents)} documents at {company_url} with JS rendering")
                    return documents
            
            # Fall back to regular request if JS rendering fails or finds no documents
            html_content = self.url_fetcher.make_request(company_url)
            if html_content:
                logger.info(f"Successfully fetched content from {company_url}")
                documents = self.document_parser.parse_documents(html_content, company_name)
                if documents:
                    logger.info(f"Found {len(documents)} documents at {company_url}")
                    return documents
        
        # If direct URLs didn't work, generate all possible URL variations and try them
        urls_to_try = self._generate_url_variations(company_id, company_name)
        
        # Remove URLs we've already tried
        urls_to_try = [url for url in urls_to_try if url not in tried_urls]
        
        # Try each URL variation
        for url in urls_to_try:
            tried_urls.add(url)
            logger.info(f"Trying URL variation: {url}")
            
            # Try with JavaScript rendering first
            html_content = self.url_fetcher.make_request(url, use_js_rendering=True)
            if html_content:
                logger.info(f"Successfully fetched content from {url} with JS rendering")
                documents = self.document_parser.parse_documents(html_content, company_name)
                if documents:
                    logger.info(f"Found {len(documents)} documents at {url} with JS rendering")
                    return documents
            
            # Fall back to regular request
            html_content = self.url_fetcher.make_request(url)
            if html_content:
                logger.info(f"Successfully fetched content from {url}")
                documents = self.document_parser.parse_documents(html_content, company_name)
                if documents:
                    logger.info(f"Found {len(documents)} documents at {url}")
                    return documents
                else:
                    logger.debug(f"No documents found at {url}, trying next variation")
        
        logger.warning(f"Could not find any documents for {company_name} after trying {len(tried_urls)} URL variations")
        return []
    
    def _parse_documents(self, html_content: str, company_name: str) -> List[Dict[str, Any]]:
        """Parse HTML to extract documents
        
        Args:
            html_content: HTML content of the company page
            company_name: Name of the company for logging
            
        Returns:
            List of document dictionaries with title, date, url, and metadata
        """
        return self.document_parser.parse_documents(html_content, company_name)
    
    def _detect_document_type(self, title: str, url: str) -> str:
        """Detect the type of document based on title and URL
        
        Args:
            title: Document title
            url: Document URL
            
        Returns:
            Document type/category as a string
        """
        return self.document_parser.detect_document_type(title, url)
    
    def _detect_country_info(self, title: str, url: str) -> Dict[str, Any]:
        """Detect country information in document title or URL
        
        Args:
            title: Document title
            url: Document URL
            
        Returns:
            Dictionary with country information or empty dict if none detected
        """
        return self.document_parser.detect_country_info(title, url)
    
    def _generate_document_id(self, title: str, url: str, date: str) -> str:
        """Generate a unique ID for a document based on its properties
        
        Args:
            title: Document title
            url: Document URL
            date: Document date
            
        Returns:
            String identifier for the document
        """
        return self.document_parser.generate_document_id(title, url, date)
    
    def test_url_extraction(self) -> Dict[str, Dict[str, str]]:
        """Test method to extract and return company URLs
        
        Returns:
            Dictionary of company IDs to URL and name information
        """
        return self.fetch_company_urls()
    
    def check_all_companies(self, company_mapping: Dict[str, str]) -> List[Dict[str, Any]]:
        """Check all companies for new documents
        
        Args:
            company_mapping: Dictionary mapping company IDs to names
            
        Returns:
            List of new documents detected
        """
        if not company_mapping:
            logger.warning("No company mapping provided")
            return []
        
        logger.info(f"Checking documents for {len(company_mapping)} companies")
        
        new_documents = []
        all_documents = []
        
        # Process each company
        for company_id, company_name in company_mapping.items():
            try:
                logger.info(f"Checking documents for {company_name} (ID: {company_id})")
                
                # Get documents for this company
                company_documents = self.get_company_documents(company_id, company_name)
                
                if company_documents:
                    logger.info(f"Found {len(company_documents)} documents for {company_name}")
                    
                    # Check if we've seen these documents before
                    previously_seen = set()
                    if company_id in self.documents_data:
                        for doc in self.documents_data[company_id]:
                            if 'id' in doc:
                                previously_seen.add(doc['id'])
                    
                    # Add to company's documents
                    if company_id not in self.documents_data:
                        self.documents_data[company_id] = []
                    
                    # Check for new documents
                    for doc in company_documents:
                        all_documents.append(doc)
                        
                        # Make sure document has an ID
                        if 'id' not in doc:
                            doc['id'] = self._generate_document_id(
                                doc.get('title', ''), 
                                doc.get('url', ''), 
                                doc.get('date', '')
                            )
                        
                        # Check if it's a new document
                        if doc['id'] not in previously_seen:
                            logger.info(f"New document found: {doc['title']} (ID: {doc['id']})")
                            
                            # Add company name to document
                            doc['company_name'] = company_name
                            
                            # Add to new documents list
                            new_documents.append(doc)
                            
                            # Add to company's documents
                            self.documents_data[company_id].append(doc)
                        else:
                            logger.debug(f"Document already seen: {doc['title']} (ID: {doc['id']})")
                else:
                    logger.info(f"No documents found for {company_name}")
            except Exception as e:
                logger.error(f"Error checking documents for {company_name}: {e}", exc_info=True)
        
        # Save updates to file
        self._save_documents_data()
        
        logger.info(f"Found {len(new_documents)} new documents across {len(company_mapping)} companies")
        return new_documents